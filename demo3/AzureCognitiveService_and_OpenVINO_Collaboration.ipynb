{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【Azure AIとIntel AIコラボ企画】\n",
    "# Azure Custom Visionでモデルを作って、OpenVINOで高速推論するサンプル\n",
    "Azureの便利なAIサービスとIntelのAIツール使ったコラボレーションサンプルです。\n",
    "このサンプルでは、Azure Cognitive Servicesの一つであるCustom Visionにて画像分類モデルを構築し、当該モデルをIntel OpenVINO ToolkitでIA上で高速推論してみます。\n",
    "\n",
    "なお、このサンプルでは犬と猫の画像を計37種類のいずれに分類するモデルを作成します。\n",
    "<table border=\"0\">\n",
    "<tr>\n",
    "<td><center>Abyssinian</center></td>\n",
    "<td><center>American pit bull</center></td>\n",
    "<td><center>Beagle</center></td>\n",
    "<td><center>Bengal</center></td>\n",
    "<td><center>Birman</center></td>\n",
    "<td><center>Boxer</center></td>\n",
    "<td><center>・・・</center></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"./images/Abyssinian_9.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/american_pit_bull_terrier_98.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/beagle_82.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Bengal_121.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/Birman_17.jpg\" width=\"112\"></td>\n",
    "<td><img src=\"./images/boxer_54.jpg\" width=\"112\"></td>\n",
    "<td><center>・・・</center><br/>(こういう感じで計37種類あります)</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "モデル作成後、まずはProtobuf形式のモデルをTensorflowを用いて推論してみます。\n",
    "続いて、同モデルをOpenVINOのIR形式に変換し、同様の画像を対象に推論してみます。\n",
    "推論性能がどの程度変化するかご体験ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前提条件：\n",
    "- Azureのアカウントを持っており、Azure Portalへのログインができること\n",
    "- Azure VM「Intel Optimized Data Science VM」を作成していること\n",
    "    - 作成手順は[こちら](https://www.intel.ai/intel-optimized-data-science-virtual-machine-azure/)を参照\n",
    "- 上記VMにIntel OpenVINO™ Toolkitの最新版をインストールしていること\n",
    "    - インストール手順は[こちら](https://docs.openvinotoolkit.org/latest/_docs_install_guides_installing_openvino_linux.html)を参照"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習：Azure Custom Vision で画像分類モデルを作成する\n",
    "https://docs.microsoft.com/ja-jp/azure/cognitive-services/custom-vision-service/getting-started-build-a-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Azure Portal](https://portal.azure.com/)へログインする\n",
    "1. ホーム画面の「リソースの作成」をクリック\n",
    "1. 検索フォームに「Custom Vision」と入力してEnter\n",
    "1. Custom Visionの作成画面が表示されるので「作成」ボタンを押下\n",
    "1. 以下のように入力\n",
    "    1. 名前：適当な名前を入力\n",
    "    1. サブスクリプション：Azure for Studentsを選択\n",
    "    1. リソースグループ：新規作成を押下して、適当な名前を入力\n",
    "    1. トレーニングの場所：（アジア太平洋）東日本　を選択\n",
    "    1. トレーニングの価格レベル： S0　を選択\n",
    "    1. 予測の場所：（アジア太平洋）東日本　を選択\n",
    "    1. 予測の価格レベル：S0　を選択\n",
    "1. 作成ボタンを押下。（少し待ちます）\n",
    "1. 作成が完了したことを確認したら、[Custom Visionのポータル](https://www.customvision.ai/)に移動\n",
    "1. 「SIGN IN」をクリックして、ログイン\n",
    "1. 「New Project」をクリック\n",
    "1. 以下のように入力\n",
    "    1. Name：適当に入力\n",
    "    1. Description：空白でOK\n",
    "    1. Resource：先ほど作成した「○○customvision」を選択\n",
    "    1. Project Types：Classification　を選択学習\n",
    "    1. Classification Types：Multiclass (Single tag per image)　を選択\n",
    "    1. Domains：General(Compact)　を選択\n",
    "    1. Export Capabilities：Basic Platform　を選択\n",
    "1. Createボタンを押下\n",
    "1. Project画面にて、「Add Images」ボタンを押下\n",
    "1. ローカルPCのディレクトリから学習用画像が入っている「Train」フォルダへ移動し、適当なカテゴリフォルダを開く\n",
    "1. カテゴリフォルダ配下の画像を全選択し、「Open」を押下\n",
    "1. Image Uploadというダイアログが開くので、「My Tags」にカテゴリ名を入力し、Upload ○○ files ボタンを押下\n",
    "1. 上記をカテゴリ数（37個）分繰り返す。または、下記スクリプトを実行して、全画像をバッチでアップロードする。\n",
    "1. 画像アップロード、および、タグ付けが完了したら画面右上のTrainボタンを押下し、Quick Trainingを選択してTrainボタンを押下\n",
    "1. しばらくしてトレーニングが完了するので、結果を確認\n",
    "1. 画面真ん中上部のExportを押下し、Tensorflowをクリック、Choose a versionでTensorflowを選択し、Exportボタンを押下、するとボタンがDownloadボタンに変わるので、それを押下すると、モデルがZIP形式でローカルPCにダウンロードされる。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （Option）画像をバッチでアップロードするためのスクリプト\n",
    "上記手順の15～16の作業を自動化するスクリプトです。必要に応じてお使いください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずはCustom VisionのSDKをインストールする必要があります\n",
    "!pip install azure-cognitiveservices-vision-customvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用の画像をバッチでCustom Visionへアップロードし、同時にタグ付け（正解ラベル付与）も行うスクリプトです\n",
    "\n",
    "from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient\n",
    "from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateEntry\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def upload_images(batch_size=64):\n",
    "    if batch_size <= 0 or batch_size > 64:\n",
    "        batch_size = 64\n",
    "        \n",
    "    # training_keyはAzure PortalのCustom Visionの画面にて、Endpointにて確認できます。皆さんの独自のエンドポイントに置き換えてください。\n",
    "    ENDPOINT = \"https://hiouchiycustomvision.cognitiveservices.azure.com/\"\n",
    "\n",
    "    # training_keyはAzure PortalのCustom Visionの画面にて、Keyにて確認できます。皆さんの独自のキーに置き換えてください。\n",
    "    training_key = \"1868b0e3ee1f414ab00a2f52b47ce6be\"\n",
    "    trainer = CustomVisionTrainingClient(training_key, endpoint=ENDPOINT)\n",
    "\n",
    "    print (\"Refering project...\")\n",
    "\n",
    "    # この文字列はCustom VisionのProject IDです。Custom Visionのポータル画面の右上の歯車マークを押して、\n",
    "    # Project Settings画面のProject IDをコピーしてください\n",
    "    project = trainer.get_project(\"9aa986ee-fda7-4bfd-a783-65c39328930c\")\n",
    "\n",
    "    print(\"Adding images...\")\n",
    "    image_list = []\n",
    "    tags_dic = {}\n",
    "    tag_list = trainer.get_tags(project.id)\n",
    "    for tag in tag_list:\n",
    "        tags_dic[tag.name] = tag.id\n",
    "    file_list = glob.glob(\"train/*/*\")\n",
    "    count = 0\n",
    "    image_count = 0\n",
    "    uploaded_images = 0\n",
    "    for file in file_list:\n",
    "        image_count = image_count + 1\n",
    "        if count < batch_size :\n",
    "            img_cat = os.path.split(os.path.dirname(file))[1]\n",
    "            if img_cat in tags_dic.keys(): \n",
    "                tag_id = tags_dic[img_cat]\n",
    "            else:\n",
    "                tag = trainer.create_tag(project.id, img_cat)\n",
    "                tag_id = tag.id\n",
    "                tags_dic[img_cat] = tag_id\n",
    "\n",
    "            with open(file, \"rb\") as image_contents:\n",
    "                image_list.append(ImageFileCreateEntry(name=file, contents=image_contents.read(), tag_ids=[tag_id]))\n",
    "            count = count + 1\n",
    "            \n",
    "            if count == batch_size or image_count == len(file_list):\n",
    "                count = 0\n",
    "                upload_result = trainer.create_images_from_files(project.id, images=image_list)\n",
    "                if not upload_result.is_batch_successful:\n",
    "                    print(\"Image batch upload failed.\")\n",
    "                    for image in upload_result.images:\n",
    "                        print(\"Image status: \", image.status)\n",
    "                    exit(-1)\n",
    "                else:\n",
    "                    uploaded_images = uploaded_images + len(image_list)\n",
    "                    print(\"Uploaded \" + str(int((uploaded_images / len(file_list))*100.0)) + \"% of total images.\")\n",
    "                image_list.clear()\n",
    "        \n",
    "    print(\"Uploaded \" + str(len(file_list)) + \" images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_images(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論①：Azureのドキュメント通りに推論スクリプトを実装して実行してみる\n",
    "https://docs.microsoft.com/ja-jp/azure/cognitive-services/custom-vision-service/export-model-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time #ここ追加\n",
    "\n",
    "def convert_to_opencv(image):\n",
    "    # RGB -> BGR conversion is performed as well.\n",
    "    image = image.convert('RGB')\n",
    "    r,g,b = np.array(image).T\n",
    "    opencv_image = np.array([b,g,r]).transpose()\n",
    "    return opencv_image\n",
    "\n",
    "def crop_center(img,cropx,cropy):\n",
    "    h, w = img.shape[:2]\n",
    "    startx = w//2-(cropx//2)\n",
    "    starty = h//2-(cropy//2)\n",
    "    return img[starty:starty+cropy, startx:startx+cropx]\n",
    "\n",
    "def resize_down_to_1600_max_dim(image):\n",
    "    h, w = image.shape[:2]\n",
    "    if (h < 1600 and w < 1600):\n",
    "        return image\n",
    "\n",
    "    new_size = (1600 * w // h, 1600) if (h > w) else (1600, 1600 * h // w)\n",
    "    return cv2.resize(image, new_size, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "def resize_to_256_square(image):\n",
    "    h, w = image.shape[:2]\n",
    "    return cv2.resize(image, (256, 256), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "def update_orientation(image):\n",
    "    exif_orientation_tag = 0x0112\n",
    "    if hasattr(image, '_getexif'):\n",
    "        exif = image._getexif()\n",
    "        if (exif != None and exif_orientation_tag in exif):\n",
    "            orientation = exif.get(exif_orientation_tag, 1)\n",
    "            # orientation is 1 based, shift to zero based and flip/transpose based on 0-based values\n",
    "            orientation -= 1\n",
    "            if orientation >= 4:\n",
    "                image = image.transpose(Image.TRANSPOSE)\n",
    "            if orientation == 2 or orientation == 3 or orientation == 6 or orientation == 7:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            if orientation == 1 or orientation == 2 or orientation == 5 or orientation == 6:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    return image\n",
    "\n",
    "def run_normal(imageFile):\n",
    "    start1 = time.time() #ここ追加\n",
    "    \n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    labels = []\n",
    "\n",
    "    # These are set to the default names from exported models, update as needed.\n",
    "    filename = \"model.pb\"\n",
    "    labels_filename = \"labels.txt\"\n",
    "\n",
    "    # Import the TF graph\n",
    "    with tf.io.gfile.GFile(filename, 'rb') as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "    # Create a list of labels.\n",
    "    with open(labels_filename, 'rt') as lf:\n",
    "        for l in lf:\n",
    "            labels.append(l.strip())\n",
    "\n",
    "    # Load from a file\n",
    "    image = Image.open(imageFile)\n",
    "\n",
    "    # Update orientation based on EXIF tags, if the file has orientation info.\n",
    "    image = update_orientation(image)\n",
    "\n",
    "    # Convert to OpenCV format\n",
    "    image = convert_to_opencv(image)\n",
    "\n",
    "    # If the image has either w or h greater than 1600 we resize it down respecting\n",
    "    # aspect ratio such that the largest dimension is 1600\n",
    "    image = resize_down_to_1600_max_dim(image)\n",
    "\n",
    "    # We next get the largest center square\n",
    "    h, w = image.shape[:2]\n",
    "    min_dim = min(w,h)\n",
    "    max_square_image = crop_center(image, min_dim, min_dim)\n",
    "\n",
    "    # Resize that square down to 256x256\n",
    "    augmented_image = resize_to_256_square(max_square_image)\n",
    "\n",
    "    # Get the input size of the model\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        input_tensor_shape = sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n",
    "    network_input_size = input_tensor_shape[1]\n",
    "\n",
    "    # Crop the center for the specified network_input_Size\n",
    "    augmented_image = crop_center(augmented_image, network_input_size, network_input_size)\n",
    "\n",
    "    # These names are part of the model and cannot be changed.\n",
    "    output_layer = 'loss:0'\n",
    "    input_node = 'Placeholder:0'\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        try:\n",
    "            prob_tensor = sess.graph.get_tensor_by_name(output_layer)\n",
    "            start2 = time.time() #ここ追加\n",
    "            predictions,  = sess.run(prob_tensor, {input_node: [augmented_image] })\n",
    "            infer_time = time.time() - start2\n",
    "        except KeyError:\n",
    "            print (\"Couldn't find classification output layer: \" + output_layer + \".\")\n",
    "            print (\"Verify this a model exported from an Object Detection project.\")\n",
    "            exit(-1)\n",
    "\n",
    "        # Print the highest probability label\n",
    "        highest_probability_index = np.argmax(predictions)\n",
    "        total_time = time.time() - start1\n",
    "        print(imageFile + ',', 'Total Time: ' + str(int(total_time*1000.0)) + 'msec,', 'Infer Time: ' + str(int(infer_time*1000.0)) + 'msec,', 'Pred Label: ' + labels[highest_probability_index]) #ここ追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#テスト用の画像をランダムに選択して推論実行\n",
    "import glob\n",
    "import random\n",
    "\n",
    "file_list = glob.glob(\"test/*/*\")\n",
    "img_path = random.choice(file_list)\n",
    "run_normal(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論②：上記推論スクリプトをリファクタリングして高速化\n",
    "前述のスクリプトだと、かなり処理が遅かったと思われます。\n",
    "ここでは、前述のスクリプトをリファクタリングすることで、高速化を図ります。（まだOpenVINOは使いません。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### まずはスーパークラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time #ここ追加\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labels = []\n",
    "        labels_filename = \"labels.txt\"\n",
    "\n",
    "        # Create a list of labels.\n",
    "        with open(labels_filename, 'rt') as lf:\n",
    "            for l in lf:\n",
    "                self.labels.append(l.strip())\n",
    "\n",
    "    def predict(self, imageFile):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def convert_to_opencv(self, image):\n",
    "        # RGB -> BGR conversion is performed as well.\n",
    "        image = image.convert('RGB')\n",
    "        r,g,b = np.array(image).T\n",
    "        opencv_image = np.array([b,g,r]).transpose()\n",
    "        return opencv_image\n",
    "\n",
    "    def crop_center(self, img,cropx,cropy):\n",
    "        h, w = img.shape[:2]\n",
    "        startx = w//2-(cropx//2)\n",
    "        starty = h//2-(cropy//2)\n",
    "        return img[starty:starty+cropy, startx:startx+cropx]\n",
    "\n",
    "    def resize_down_to_1600_max_dim(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if (h < 1600 and w < 1600):\n",
    "            return image\n",
    "\n",
    "        new_size = (1600 * w // h, 1600) if (h > w) else (1600, 1600 * h // w)\n",
    "        return cv2.resize(image, new_size, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    def resize_to_256_square(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        return cv2.resize(image, (256, 256), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    def update_orientation(self, image):\n",
    "        exif_orientation_tag = 0x0112\n",
    "        if hasattr(image, '_getexif'):\n",
    "            exif = image._getexif()\n",
    "            if (exif != None and exif_orientation_tag in exif):\n",
    "                orientation = exif.get(exif_orientation_tag, 1)\n",
    "                # orientation is 1 based, shift to zero based and flip/transpose based on 0-based values\n",
    "                orientation -= 1\n",
    "                if orientation >= 4:\n",
    "                    image = image.transpose(Image.TRANSPOSE)\n",
    "                if orientation == 2 or orientation == 3 or orientation == 6 or orientation == 7:\n",
    "                    image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                if orientation == 1 or orientation == 2 or orientation == 5 or orientation == 6:\n",
    "                    image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 続いて、Tensorflowモデルのサブクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time #ここ追加\n",
    "\n",
    "class TFModel(Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TFModel, self).__init__()\n",
    "        \n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "\n",
    "        # These are set to the default names from exported models, update as needed.\n",
    "        filename = \"model.pb\"\n",
    "\n",
    "        # Import the TF graph\n",
    "        with tf.io.gfile.GFile(filename, 'rb') as f:\n",
    "            graph_def.ParseFromString(f.read())\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session()\n",
    "                \n",
    "    def predict(self, imageFile):\n",
    "        start1 = time.time() #ここ追加\n",
    "\n",
    "        # Load from a file\n",
    "        image = Image.open(imageFile)\n",
    "\n",
    "        # Update orientation based on EXIF tags, if the file has orientation info.\n",
    "        image = super().update_orientation(image)\n",
    "\n",
    "        # Convert to OpenCV format\n",
    "        image = super().convert_to_opencv(image)\n",
    "\n",
    "        # If the image has either w or h greater than 1600 we resize it down respecting\n",
    "        # aspect ratio such that the largest dimension is 1600\n",
    "        image = super().resize_down_to_1600_max_dim(image)\n",
    "\n",
    "        # We next get the largest center square\n",
    "        h, w = image.shape[:2]\n",
    "        min_dim = min(w,h)\n",
    "        max_square_image = super().crop_center(image, min_dim, min_dim)\n",
    "\n",
    "        # Resize that square down to 256x256\n",
    "        augmented_image = super().resize_to_256_square(max_square_image)\n",
    "\n",
    "        # Get the input size of the model\n",
    "        input_tensor_shape = self.sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n",
    "        network_input_size = input_tensor_shape[1]\n",
    "\n",
    "        # Crop the center for the specified network_input_Size\n",
    "        augmented_image = super().crop_center(augmented_image, network_input_size, network_input_size)\n",
    "        frame = augmented_image\n",
    "\n",
    "        # These names are part of the model and cannot be changed.\n",
    "        output_layer = 'loss:0'\n",
    "        input_node = 'Placeholder:0'\n",
    "\n",
    "        try:\n",
    "            prob_tensor = self.sess.graph.get_tensor_by_name(output_layer)\n",
    "            start2 = time.time() #ここ追加\n",
    "            predictions, = self.sess.run(prob_tensor, {input_node: [augmented_image] })\n",
    "            infer_time = time.time() - start2\n",
    "        except KeyError:\n",
    "            print (\"Couldn't find classification output layer: \" + output_layer + \".\")\n",
    "            print (\"Verify this a model exported from an Object Detection project.\")\n",
    "            exit(-1)\n",
    "\n",
    "        # Print the highest probability label\n",
    "        highest_probability_index = np.argmax(predictions)\n",
    "        total_time = time.time() - start1\n",
    "        \n",
    "        return total_time, infer_time, self.labels[highest_probability_index], frame  #ここ追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論用関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def run_inference(model_type, target_device='CPU', total=50):\n",
    "    if model_type == 'tf':\n",
    "        model = TFModel()\n",
    "    else:\n",
    "        if target_device == 'GPU':\n",
    "            model = OpenVINOModel('GPU')\n",
    "        elif target_device == 'MYRIAD':\n",
    "            model = OpenVINOModel('MYRIAD')\n",
    "        else:\n",
    "            model = OpenVINOModel('CPU')\n",
    "\n",
    "    total_infer_spent_time = 0\n",
    "    total_spent_time = 0\n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    for i in range(total):\n",
    "        file_list = glob.glob(\"test/*/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        total_time, infer_time, pred_label, frame = model.predict(img_path)\n",
    "\n",
    "        total_infer_spent_time += infer_time\n",
    "        total_spent_time += total_time\n",
    "\n",
    "        #print(img_path, str(int(total_time*1000.0)) + 'msec', str(int(infer_time*1000.0)) + 'msec', pred_label) #ここ追加\n",
    "        clear_output(wait=True)\n",
    "        cv2.putText(frame, str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame, str(i) + ':', (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame, str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame, str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame, str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame, str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(total_time * 1000)), str(int(infer_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True ) \n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    display(list_df)\n",
    "\n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_total_time, tf_infer_time = run_inference('tf', total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推論③：Intel OpenVINO™ Toolkitを使うことで更に高速化\n",
    "ひとつ前の改変により、Tensorflow上であってもモデルの推論処理がそれなりに高速化しました。\n",
    "ここでは、更なる高速化のためにIntel OpenVINO™ Toolkitを使ってみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずはProtobuf形式のモデルをOpenVINOのIR形式に変換\n",
    "\n",
    "OpenVINOに付属しているModel Optimizerを使います。\n",
    "\n",
    "より詳しい説明は下記URLをご覧ください。 https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Converting_Model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model=model.pb --input_shape=[1,224,224,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この後で同モデルをNeural Compute Stick2上で実行するため、FP16形式にも変換しておきます。（NCS2はFP16のみ対応のため）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model=model.pb --input_shape=[1,224,224,3] --data_type=FP16 --model_name=model.fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 続いて、OpenVINOモデルのサブクラスを記述"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time #ここ追加\n",
    "from openvino.inference_engine import IENetwork, IEPlugin #ここ追加\n",
    "\n",
    "class OpenVINOModel(Model):\n",
    "\n",
    "    def __init__(self, target_device):\n",
    "        super(OpenVINOModel, self).__init__()\n",
    "\n",
    "        # These are set to the default names from exported models, update as needed.\n",
    "        model_xml = 'model.xml'\n",
    "        model_bin = 'model.bin'\n",
    "\n",
    "        # Plugin initialization for specified device and load extensions library if specified\n",
    "        # Set the desired device name as 'device' parameter. This sample support these 3 names: CPU, GPU, MYRIAD\n",
    "        ie = IEPlugin(device=target_device, plugin_dirs='')\n",
    "\n",
    "        # Read IR\n",
    "        self.net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "        self.input_blob = next(iter(self.net.inputs))\n",
    "        self.out_blob = next(iter(self.net.outputs))\n",
    "        self.net.batch_size = 1\n",
    "\n",
    "        # Loading model to the plugin\n",
    "        self.exec_net = ie.load(network=self.net)\n",
    "\n",
    "    def predict(self, imageFile):\n",
    "        start1 = time.time() #ここ追加\n",
    "\n",
    "        # Load from a file\n",
    "        image = Image.open(imageFile)\n",
    "\n",
    "        # Update orientation based on EXIF tags, if the file has orientation info.\n",
    "        image = super().update_orientation(image)\n",
    "\n",
    "        # Convert to OpenCV format\n",
    "        image = super().convert_to_opencv(image)\n",
    "\n",
    "        # If the image has either w or h greater than 1600 we resize it down respecting\n",
    "        # aspect ratio such that the largest dimension is 1600\n",
    "        image = super().resize_down_to_1600_max_dim(image)\n",
    "\n",
    "        # We next get the largest center square\n",
    "        h, w = image.shape[:2]\n",
    "        min_dim = min(w,h)\n",
    "        max_square_image = super().crop_center(image, min_dim, min_dim)\n",
    "\n",
    "        # Resize that square down to 256x256\n",
    "        augmented_image = super().resize_to_256_square(max_square_image)\n",
    "\n",
    "        # Get the input size of the model\n",
    "        n, c, h, w = self.net.inputs[self.input_blob].shape\n",
    "\n",
    "        # Crop the center for the specified network_input_Size\n",
    "        augmented_image = super().crop_center(augmented_image, w, h)\n",
    "        frame = augmented_image\n",
    "\n",
    "        #\n",
    "        augmented_image = augmented_image.transpose((2, 0, 1))\n",
    "\n",
    "        images = np.ndarray(shape=(n, c, h, w))\n",
    "        images[0] = augmented_image\n",
    "\n",
    "        start2 = time.time() #ここ追加\n",
    "        predictions = self.exec_net.infer(inputs={self.input_blob: images})\n",
    "        infer_time = time.time() - start2\n",
    "\n",
    "        # Print the highest probability label\n",
    "        predictions = predictions[self.out_blob]\n",
    "        highest_probability_index = predictions[0].argsort()[-1:][::-1]\n",
    "\n",
    "        total_time = time.time() - start1\n",
    "\n",
    "        return total_time, infer_time, self.labels[highest_probability_index[0]], frame  #ここ追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO (on CPU)で推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_total_time, cpu_infer_time = run_inference('openvino', target_device='CPU', total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO (on Integrated GPU)で推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_total_time, gpu_infer_time = run_inference('openvino', target_device='GPU', total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO (on Neural Compute Stick2)で推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncs_total_time, ncs_infer_time = run_inference('openvino', target_device='CPU', total=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## それぞれの結果をグラフ化して比較してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w = 0.4\n",
    "\n",
    "Y1 = [tf_total_time - tf_infer_time, cpu_total_time - cpu_infer_time, gpu_total_time - gpu_infer_time, ncs_total_time - ncs_infer_time]\n",
    "Y2 = [tf_infer_time, cpu_infer_time, gpu_infer_time, ncs_infer_time]\n",
    "\n",
    "X = np.arange(len(Y1))\n",
    "\n",
    "plt.bar(X, Y1, color='gray', width=w, label='Pre/Post', align=\"center\")\n",
    "plt.bar(X, Y2, color='blue', width=w, bottom=Y1, label='Inference', align=\"center\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title('MobileNet Performance Comparison')\n",
    "plt.ylabel(\"Spent time per one image (msec)\")\n",
    "\n",
    "plt.xticks(X, ['Keras(CPU)','OpenVINO(CPU)','OpenVINO(iGPU)','OpenVINO(NCS2)'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
